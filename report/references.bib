@article{text2image,
	title = {Generative {Adversarial} {Text} to {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1605.05396},
	abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
	urldate = {2016-12-19},
	journal = {arXiv:1605.05396 [cs]},
	author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
	month = may,
	year = {2016},
	note = {arXiv: 1605.05396},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: ICML 2016},
	file = {1605.05396v2.pdf:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/A7CI2ZEF/1605.05396v2.pdf:application/pdf;arXiv\:1605.05396 PDF:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/IX69V6PC/Reed et al. - 2016 - Generative Adversarial Text to Image Synthesis.pdf:application/pdf;arXiv.org Snapshot:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/MNPG2PS3/1605.html:text/html}
}

@article{infogan,
	title = {{InfoGAN}: {Interpretable} {Representation} {Learning} by {Information} {Maximizing} {Generative} {Adversarial} {Nets}},
	shorttitle = {{InfoGAN}},
	url = {http://arxiv.org/abs/1606.03657},
	abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
	urldate = {2016-12-19},
	journal = {arXiv:1606.03657 [cs, stat]},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03657},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1606.03657 PDF:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/KZGQS8GX/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by .pdf:application/pdf;arXiv.org Snapshot:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/G7RHDTZ7/1606.html:text/html}
}

@article{salimans_improved_2016,
	title = {Improved {Techniques} for {Training} {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	urldate = {2016-12-19},
	journal = {arXiv:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03498},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1606.03498 PDF:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/97MIEEMK/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf;arXiv.org Snapshot:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/GI6V5DP6/1606.html:text/html}
}

@techreport{cub,
	title = {Caltech-{UCSD} {Birds} 200},
	url = {http://www.vision.caltech.edu/visipedia/CUB-200.html},
	urldate = {2016-12-23},
	file = {Caltech-UCSD Birds 200:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/IX7GPGN9/CUB-200.html:text/html;WelinderEtal10_CUB-200.pdf:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/GVMUDM5H/WelinderEtal10_CUB-200.pdf:application/pdf}
}

@misc{flowers,
	title = {Visual {Geometry} {Group} {Home} {Page}},
	url = {http://www.robots.ox.ac.uk/~vgg/data/flowers/102/},
	urldate = {2016-12-23},
	file = {Visual Geometry Group Home Page:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/4T9Z8IAC/102.html:text/html}
}

@article{gan,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2016-12-24},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1406.2661 PDF:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/ISNI2Q3V/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/23GI9WBJ/1406.html:text/html}
}

@article{dcgan,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2016-12-24},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {arXiv\:1511.06434 PDF:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/43DPP7HV/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/U3W53CPE/1511.html:text/html}
}

@article{batchnorm,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2016-12-24},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1502.03167 PDF:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/NZ8Q7XPJ/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/N6C6HSDE/1502.html:text/html}
}

@article{visualdesc,
	title = {Learning {Deep} {Representations} of {Fine}-grained {Visual} {Descriptions}},
	url = {http://arxiv.org/abs/1605.05395},
	abstract = {State-of-the-art methods for zero-shot visual recognition formulate learning as a joint embedding problem of images and side information. In these formulations the current best complement to visual features are attributes: manually encoded vectors describing shared characteristics among categories. Despite good performance, attributes have limitations: (1) finer-grained recognition requires commensurately more attributes, and (2) attributes do not provide a natural language interface. We propose to overcome these limitations by training neural language models from scratch; i.e. without pre-training and only consuming words and characters. Our proposed models train end-to-end to align with the fine-grained and category-specific content of images. Natural language provides a flexible and compact way of encoding only the salient visual aspects for distinguishing categories. By training on raw text, our model can do inference on raw text as well, providing humans a familiar mode both for annotation and retrieval. Our model achieves strong performance on zero-shot text-based image retrieval and significantly outperforms the attribute-based state-of-the-art for zero-shot classification on the Caltech UCSD Birds 200-2011 dataset.},
	urldate = {2016-12-24},
	journal = {arXiv:1605.05395 [cs]},
	author = {Reed, Scott and Akata, Zeynep and Schiele, Bernt and Lee, Honglak},
	month = may,
	year = {2016},
	note = {arXiv: 1605.05395},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2016},
	file = {arXiv\:1605.05395 PDF:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/W9X9723F/Reed et al. - 2016 - Learning Deep Representations of Fine-grained Visu.pdf:application/pdf;arXiv.org Snapshot:/home/srivas/.zotero/zotero/p7wbxgou.default/zotero/storage/CUT5IJBV/1605.html:text/html}
}
