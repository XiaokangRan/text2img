\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[nonatbib,final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{physics}		 % Physics package for derivatives
\usepackage{graphicx,float,caption,subcaption,tikz}
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white

\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{array}
\captionsetup[table]{skip=10pt}

\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\E}{\mathbb{E}}

\usepackage[backend=bibtex,sorting=none,autocite=superscript]{biblatex}

\DeclareCiteCommand{\supercite}[\mkbibsuperscript]
  {\iffieldundef{prenote}
     {}
     {\BibliographyWarning{Ignoring prenote argument}}%
   \iffieldundef{postnote}
     {}
     {\BibliographyWarning{Ignoring postnote argument}}}
  {\usebibmacro{citeindex}%
   \bibopenbracket\usebibmacro{cite}\bibclosebracket}
  {\supercitedelim}
  {}
  
\bibliography{references}
\let\cite=\supercite
\graphicspath{ {images/} }


\title{Computer Vision Report: Text to Image Synthesis}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Ankit Vani \\
  \texttt{ankit.vani@nyu.edu} \\
  \And
  Srivas Venkatesh \\
  \texttt{srivas.venkatesh@nyu.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
  Generation of realistic images given a text describing it is a challenging and interesting problem. With the recent successes of Generative Adversarial Networks in this field, we aim to explore it and use information theoretical extension to it to isolate meaningful latent variables.
\end{abstract}


\section{Introduction}


\section{Background}

\subsection{Generative Adversarial Network (GAN)}

Generative Adversarial Networks (GANs) \cite{gan} have been tremendously successful in learning to generate realistic images by playing a min-max game. A discriminator $D$ is trained to classify input images as coming from the data distribution (`real') or coming from the generator (`fake'). A generator $G$ is trained to fool the discriminator by producing images that are indistinguishable from real images for the discriminator. The generator takes as input a noise vector $z$, coming from a prior distribution, and maps it to a point in the pixel space, which represents an image. The prior noise distribution usually has zero mean and is a Gaussian or a uniform distribution.

The GAN objective can thus be formulated as
\begin{align}
\min_G \max_D\, V(D,G) = \E_{x \sim p_{\text{data}}(x)}\left[\log D(x)\right] + \E_{z\sim p_z(z)}\left[\log(1-D(G(z)))\right] \label{eq:gan}
\end{align}

The signal for the generator to update its generative model comes from the discriminator itself, by using gradients of the discriminator loss with respect to the generated image. In practice, gradients from the discriminator are reversed and backpropagated to the generator, thereby asking the generator to maximize the discriminator's loss.


\subsection{Deep Convolutional GAN (DCGAN)}

To get better quality generations from the generators of GANs, \cite{dcgan} presented an architecture called deep convolutional GAN (DCGAN) where the generator and the discriminator are convolutional networks that mirror each other in structure.

They also replace the pooling layers with strided convolutions in the discriminator and fractional-strided convolutions in the generator. Additionally, they use the LeakyReLU activation in the discriminator for all layers instead of ReLU. These two additions ensure that there are no sparse gradients in the discriminator, which helps with the stability of the min-max objective training. To alleviate internal covariate shift and help make training more stable, they use batch normalization \cite{batchnorm} in both the generator and the discriminator. There are no fully connected hidden layers, enabling deeper models.

The text to image architecture we use is based on the DCGAN architecture.



\subsection{Information Maximizing GAN (InfoGAN)}

Information maximizing GAN (InfoGAN)\cite{infogan} is an extension to GAN that learns disentangled representations in an unsupervised manner. Unlike the standard GAN, where an image is generated from a noise vector $z$, an image in this case is generated from a noise vector $z$ and latent codes $c$. Like $z$, we define a prior distribution over $c$ with zero mean, which is usually taken to be a Gaussian or a uniform distribution. $c$ can also contain discrete latent codes, which can have a discrete uniform prior.

To learn disentangled representations, the model maximizes the mutual information between $c$ and the generated image $G(z,c)$, where the mutual information is given by
\begin{align}
\mathcal{I}(c;G(z,c)) = \mathcal{H}(G(z,c)) - \mathcal{H}(G(z,c) \mid c) = \mathcal{H}(c) - \mathcal{H}(c\mid G(z,c)) \label{eq:mi}
\end{align}

The authors derive a variational lower bound to Equation (\ref{eq:mi}):
\begin{align}
\mathcal{I}(c;G(z,c)) &\geq \E_{x\sim G(z,c)}\left[\E_{c'\sim p(c\mid x)}\left[\log Q(c'\mid x)\right]\right] + \mathcal{H}(c) \label{eq:milb}
\end{align}
where $Q$ is a variational approximation for $p(c\mid x)$. They show that under suitable regularity conditions, Equation (\ref{eq:milb}) can be simplified to
\begin{align}
\mathcal{I}(c;G(z,c)) &\geq \E_{c\sim p(c), x\sim G(z,c)}\left[\log Q(c\mid x)\right] + \mathcal{H}(c) = L_I(G,Q) \label{eq:miloss}
\end{align}

Thus, by maximizing $L_I(G,Q)$, we would maximize the mutual information between $c$ and $G(z,c)$. $L_I$ can be added to the GAN's objective and maximized jointly with respect to $G$ and $Q$ as
\begin{align}
\min_{G,Q} \max_D\, V_{\text{InfoGAN}}(D,G,Q) = V(D,G) - \lambda L_I(G,Q)
\end{align}
where $V(D,G)$ is defined in Equation (\ref{eq:gan}). $\lambda$ is a hyperparameter, that is tuned to weigh the mutual information criterion in the learning objective. The authors suggest setting this value such that $\lambda L_I$ is on the same scale as the GAN objective $V$.

In practice, $Q$ shares all of its layers and parameters with $D$ upto $D$'s penultimate layer, following with an additional layer that outputs the sufficient statistics of the distribution $Q(c\mid x)$. For a Gaussian, these are the mean and diagonal covariance values. For discrete latent codes, these are the softmax logits. $Q$ converges very quickly during training, and thus it essentially comes for free with GAN, without the need for additional training time or model capacity beyond a single layer.


\subsection{Deep symmetric structured joint embedding}

Given an image $v\in\mathcal{V}$ and its corresponding fine-grained textual description $t\in\mathcal{T}$, \cite{visualdesc} suggest learning the encodings $\theta(v)$ for images and $\varphi(t)$ for text, such that a joint embedding $F(v,t)=\theta(v)^\top \varphi(t)$ can be defined. They define the classifiers
\begin{align}
f_v(v) &= \argmax_{y\in\mathcal{Y}} \E_{t\in\mathcal{T}(y)}\left[F(v,t)\right],\\
f_t(t) &= \argmax_{y\in\mathcal{Y}} \E_{v\in\mathcal{V}(y)}\left[F(v,t)\right]
\end{align}
to classify data points $(v,t)$ into labels $y\in\mathcal{Y}$. By jointly training the text and image encoders to minimize the classification loss, they are able to learn visually descriptive embeddings $\varphi(t)$ for text and embeddings $\theta(v)$ for images, such that they have a higher compatibility score $F$ when their class is the same.

They explore various text encoder models, such as a convolutional network, a convolutional recurrent network and a recurrent network. For our work on text to image synthesis, we use pretrained text encoders provided by the authors, that use a character-level convolutional recurrent network.


\section{Text to Image InfoGAN}

\subsection{Objective}

Text to Image GAN\cite{text2image} is a conditional GAN, where the discriminator $D$ and the generator $G$ are conditioned on a text encoding $\varphi(t)$ for an image caption $t$. Thus, given a text caption, the generator is tasked with producing a realistic image described by the input caption. When training the generator, the authors also propose creating auxiliary text embeddings by interpolating between embeddings of text descriptions in the data, essentially providing infinite samples from the text embedding space to condition on.

Based on Equation (\ref{eq:gan}), we can formulate the text to image GAN objective as
\begin{align}
\min_G \max_D\, \mathcal{V}(D,G, \varphi) =& \E_{(x,t) \sim p_{\text{data}}(x,t)}\left[\log D(x, \varphi(t))\right]\, + \nonumber\\
&\E_{z\sim p_z(z), t\sim p_{\text{intdata}}(t)}\left[\log(1-D(G(z, \varphi(t)), \varphi(t)))\right] \label{eq:t2i}
\end{align}
where $t \sim p_{\text{intdata}}(t)$ is short for sampling through the process $t_1 \sim p_{\text{data}}(t), t_2 \sim p_{\text{data}}(t), \beta \sim [0,1], t = \beta t_1 + (1-\beta) t_2$.

We extend the text to image GAN by adding the ability to manipulate the output images, adjusting it manually to possibly better suit the textual description. We learn disentangled latent codes $c$, enabling each latent code to take on interpretable and semantically meaningful roles. We do this by extending InfoGAN to the text to image synthesis setting, where we want to maximize the mutual information between the latent codes $c$ and the conditionally generated image $G(z,c,\varphi(t))$:
\begin{align}
\mathcal{I}(c;G(z,c,\varphi(t))) &\geq \E_{t\sim p_{\text{intdata}}(t), x\sim G(z,c,\varphi(t))}\left[\E_{c'\sim p(c\mid x,t)}\left[\log Q(c'\mid x,t)\right]\right] + \mathcal{H}(c)
\end{align}

As described for InfoGAN, under suitable regularity conditions, we can obtain a variational lower bound for $\mathcal{I}(c;G(z,c,\varphi(t)))$:
\begin{align}
\mathcal{I}(c;G(z,c, \varphi(t))) &\geq \E_{c\sim p(c), t\sim p_{\text{intdata}}(t), x\sim G(z,c, \varphi(t))}\left[\log Q(c\mid x,t)\right] + \mathcal{H}(c) = \mathcal{L}_I(G,Q,\varphi) \label{eq:t2imiloss}
\end{align}

We experiment with two possible models, one where the variational approximation $Q$ takes into consideration the textual description when looking at the generated image, and one where it doesn't. In the case where $Q$ does not consider the textual description, we assume in Equation (\ref{eq:t2imiloss}) that the output of $Q$ is conditionally independent of $t$ given $x$, in which case $Q(c\mid x,t)$ simplifies to $Q(c\mid x)$. This is equivalent to removing an edge between $t$ and $c$ sampled from $Q$ in the graphical model.

Our text to image InfoGAN objective is given by
\begin{align}
\min_{G,Q} \max_D\, \mathcal{V}_{\text{t2i2GAN}}(D,G,Q,\varphi) = \mathcal{V}(D,G,\varphi) - \lambda \mathcal{L}_I(G,Q,\varphi)
\end{align}
where $\mathcal{V}(D,G,\varphi)$ is defined in Equation (\ref{eq:t2i}), and $\mathcal{L}_I(G,Q,\varphi)$ in Equation (\ref{eq:t2imiloss}).


\subsection{Model architecture}





\section{Code Used}

\section{Experimental Results}

\section{Conclusion}

\printbibliography

\end{document}
